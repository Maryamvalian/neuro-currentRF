{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we use the auditory Brainstorm tutorial dataset [1]_ that is available as a part of the Brainstorm software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First comes the imports for data i/o and processing, visualization etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import mne\n",
    "from mne.datasets.brainstorm import bst_auditory\n",
    "from mne.io import read_raw_ctf\n",
    "\n",
    "import eelbrain\n",
    "from eelbrain import load, plot\n",
    "\n",
    "from ncrf import fit_ncrf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Preporcess MEG Data, i.e. low pass filtering, power line attenuation, downsampling etc. \n",
    "We broadly follow [this tutorial](https://www.nmr.mgh.harvard.edu/mne/stable/auto_tutorials/sample-datasets/plot_brainstorm_auditory.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "use_precomputed = True \n",
    "\n",
    "data_path = bst_auditory.data_path()\n",
    "\n",
    "raw_fname1 = os.path.join(data_path, 'MEG', 'bst_auditory',\n",
    "                     'S01_AEF_20131218_01.ds')\n",
    "# raw_fname2 = os.path.join(data_path, 'MEG', 'bst_auditory',\n",
    "#                      'S01_AEF_20131218_02.ds')\n",
    "raw_AEF = read_raw_ctf(raw_fname1, preload=False)\n",
    "mne.rename_channels(raw_AEF.info, lambda x: x.split('-') [0])\n",
    "\n",
    "n_times_run1 = raw_AEF.n_times\n",
    "# mne.io.concatenate_raws([raw_AEF, read_raw_ctf(raw_fname2, preload=False)])\n",
    "# raw_AEF.plot()\n",
    "\n",
    "# We mark a set of bad channels that seem noisier than others. \n",
    "# The marked channels are added as bad.\n",
    "\n",
    "raw_AEF.info['bads'] = ['MLO52', 'MRT51', 'MLO42', 'MLO43']\n",
    "\n",
    "annotations_df = pd.DataFrame()\n",
    "offset = n_times_run1\n",
    "for idx in [1]:\n",
    "    csv_fname = os.path.join(data_path, 'MEG', 'bst_auditory',\n",
    "                        'events_bad_0%s.csv' % idx)\n",
    "    df = pd.read_csv(csv_fname, header=None,\n",
    "                     names=['onset', 'duration', 'id', 'label'])\n",
    "    print('Events from run {0}:'.format(idx))\n",
    "    print(df)\n",
    "\n",
    "    df['onset'] += offset * (idx - 1)\n",
    "    annotations_df = pd.concat([annotations_df, df], axis=0)\n",
    "\n",
    "# Conversion from samples to times:\n",
    "onsets = annotations_df['onset'].values / raw_AEF.info['sfreq']\n",
    "durations = annotations_df['duration'].values / raw_AEF.info['sfreq']\n",
    "descriptions = annotations_df['label'].values\n",
    "\n",
    "annotations = mne.Annotations(onsets, durations, descriptions)\n",
    "raw_AEF.set_annotations(annotations)\n",
    "del onsets, durations, descriptions\n",
    "\n",
    "\n",
    "# events are the presentation times of the audio stimuli: UPPT001\n",
    "events = mne.find_events(raw_AEF, stim_channel='UPPT001')\n",
    "# The event timing is adjusted by comparing the trigger times on detected sound onsets on channel UADC001-4408.\n",
    "sound_data = raw_AEF[raw_AEF.ch_names.index('UADC001')][0][0]\n",
    "onsets = np.where(np.abs(sound_data) > 2. * np.std(sound_data))[0]\n",
    "min_diff = int(0.5 * raw_AEF.info['sfreq'])\n",
    "diffs = np.concatenate([[min_diff + 1], np.diff(onsets)])\n",
    "onsets = onsets[diffs > min_diff]\n",
    "assert len(onsets) == len(events)\n",
    "diffs = 1000. * (events[:, 0] - onsets) / raw_AEF.info['sfreq']\n",
    "print('Trigger delay removed (Î¼ Â± Ïƒ): %0.1f Â± %0.1f ms'\n",
    "      % (np.mean(diffs), np.std(diffs)))\n",
    "\n",
    "# events times are rescaled according to new sampling freq, 100 Hz\n",
    "events[:, 0] = np.int64(onsets * 100 / raw_AEF.info['sfreq'])\n",
    "event_fname = os.path.join(data_path, 'MEG', 'bst_auditory',\n",
    "                     'S01_AEF_20131218_01-eve.fif')\n",
    "mne.write_events(event_fname, events, overwrite=True)\n",
    "\n",
    "del sound_data, diffs\n",
    "\n",
    "## set EOG channel\n",
    "# raw.set_channel_types({'EEG058': 'eog'})\n",
    "raw_AEF.set_eeg_reference('average', projection=True)\n",
    "# raw_AEF.plot_psd(tmax=60., average=False)\n",
    "raw_AEF.load_data()\n",
    "raw_AEF.notch_filter(np.arange(60, 181, 60), fir_design='firwin')\n",
    "\n",
    "# band pass filtering 1-8 Hz\n",
    "raw_AEF.filter(1.0, 8.0, fir_design='firwin')\n",
    "\n",
    "# resample to 100 Hz\n",
    "raw_AEF.resample(100, npad=\"auto\")\n",
    "\n",
    "### LOAD RELEVANT VARIABLES AS eelbrain.NDVar\n",
    "# load as epochs for plot only\n",
    "ds = load.fiff.events(raw=raw_AEF, proj=True, stim_channel='UPPT001', events=event_fname)\n",
    "epochs = load.fiff.epochs(ds, tmin=-0.1, tmax=0.5, baseline=(None, 0))\n",
    "plot.Butterfly(epochs)\n",
    "\n",
    "# pick MEG channels\n",
    "picks = mne.pick_types(raw_AEF.info, meg=True, eeg=False, stim=False, eog=False,\n",
    "                       ref_meg=False, exclude='bads')\n",
    "\n",
    "# Read as a single chunk of data\n",
    "y, t = raw_AEF.get_data(picks, return_times=True)\n",
    "sensor_dim = load.fiff.sensor_dim(raw_AEF.info, picks=picks)\n",
    "time = eelbrain.UTS.from_int(0, t.size-1, raw_AEF.info['sfreq'])\n",
    "meg = eelbrain.NDVar(y, dims=(sensor_dim, time))\n",
    "print(meg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous stimulus variable construction\n",
    "After loading and processing the raw data, we will construct the predictor variable for this particular experiment (by putting an impulse at every event time-point). Note that, the predictor variable and meg response should be of same length. \n",
    "\n",
    "In case of repetative trials (where you will have `eelbrain.cases`), supply one predictor variable for each trial. Different perdictor variables for a single trial can be nested (See README).    \n",
    "\n",
    "In this example we use two different predictor variables for a single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the common response we put impulses at the the presentation times of both the audio stimuli (i.e. beeps).\n",
    "stim1 = np.zeros(len(time), dtype=np.double)\n",
    "stim1[events[:, 0]] = 1.\n",
    "\n",
    "# To distingusih between standard and deviant beeps, we assign 1 and -1 impulses respectively.  \n",
    "stim2 = stim1.copy()\n",
    "stim2[events[np.where(events[:, 2] == 2), 0]] = -1.\n",
    "stim1 = eelbrain.NDVar(stim1, time)\n",
    "stim2 = eelbrain.NDVar(stim2, time)\n",
    "\n",
    "# visualize the stimulus\n",
    "s = plot.LineStack(eelbrain.combine([stim1, stim2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise covariance estimation\n",
    "Now we shall estimate the noise covariance from empty room data. Dont forget to apply the same pre-processing steps to empty room data.\n",
    "instead you can choose to use pre-stimulus recordings to compute noise covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noise_path = (data_path / 'MEG/bst_auditory' /\n",
    "            'S01_Noise_20131218_01.ds')\n",
    "raw_empty_room = read_raw_ctf(Noise_path, preload=True)\n",
    "mne.rename_channels(raw_empty_room.info, lambda x: x.split('-') [0])\n",
    "\n",
    "# Apply the same pre-processing steps to empty room data\n",
    "# raw_empty_room.info['bads'] = [\n",
    "#     bb for bb in raw_AEF.info['bads'] if 'EEG' not in bb]\n",
    "# raw_empty_room.add_proj(\n",
    "#     [pp.copy() for pp in raw_AEF.info['projs'] if 'EEG' not in pp['desc']])\n",
    "\n",
    "# raw_empty_room.plot_psd(tmax=60., average=False)\n",
    "raw_empty_room.notch_filter(np.arange(60, 181, 60), fir_design='firwin')\n",
    "\n",
    "raw_empty_room.filter(1.0, 8.0, fir_design='firwin')\n",
    "\n",
    "raw_empty_room.resample(100, npad=\"auto\")\n",
    "\n",
    "# copute noise covariance matrix\n",
    "noise_cov = mne.compute_raw_covariance(raw_empty_room, tmin=0, tmax=None, method='shrunk', rank=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Modelling aka lead-field matrix\n",
    "Now is the time for forward modelling. 'ico-4' or 'vol-10' sould be enough if working with surface source space.\n",
    "You can choose to work with free / constrained lead field. fit_ncrf will choose appropiate regularizer by looking at the provided lead-field matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths to Freesurfer reconstructions\n",
    "subjects_dir = data_path / 'subjects'\n",
    "subject = 'bst_auditory'\n",
    "\n",
    "# mne.viz.plot_bem(subject=subject, subjects_dir=subjects_dir,\n",
    "#                  brain_surfaces='white', orientation='coronal')\n",
    "\n",
    "# The transformation file obtained by coregistration\n",
    "trans = data_path / 'MEG' / 'bst_auditory' / 'bst_auditory-trans.fif'\n",
    "\n",
    "info = raw_AEF.info\n",
    "# Here we look at the head only.\n",
    "# mne.viz.plot_alignment(info, trans, subject=subject, dig=True,\n",
    "#                        meg=['helmet', 'sensors'], subjects_dir=subjects_dir,\n",
    "#                        surfaces='head')\n",
    "\n",
    "srcfile = data_path / 'subjects' / 'bst_auditory' / 'bem'/ 'bst_auditory-ico-4-src.fif'\n",
    "if os.path.isfile(srcfile):\n",
    "    src = mne.read_source_spaces(srcfile)\n",
    "else:\n",
    "    src = mne.setup_source_space(subject, spacing='ico4',\n",
    "                                 subjects_dir=subjects_dir, add_dist=False)\n",
    "    mne.add_source_space_distances(src)\n",
    "    mne.write_source_spaces(srcfile, src, overwrite=True)  # needed for smoothing\n",
    "print(src)\n",
    "\n",
    "fwdfile = data_path / 'subjects' / 'bst_auditory' / 'bem' / 'bst_auditory-ico-4-fwd.fif'\n",
    "if os.path.isfile(fwdfile):\n",
    "    fwd = mne.read_forward_solution(fwdfile)\n",
    "else:\n",
    "    ## Compute Forward Solution\n",
    "    conductivity = (0.3,)  # for single layer\n",
    "    # conductivity = (0.3, 0.006, 0.3)  # for three layers\n",
    "    model = mne.make_bem_model(subject=subject, ico=4,\n",
    "                               conductivity=conductivity,\n",
    "                               subjects_dir=subjects_dir)\n",
    "    bem = mne.make_bem_solution(model)\n",
    "\n",
    "    fwd = mne.make_forward_solution(info, trans=trans, src=src, bem=bem,\n",
    "                                    meg=True, eeg=False, mindist=5.0, n_jobs=2)\n",
    "    mne.write_forward_solution(fwdfile, fwd)\n",
    "\n",
    "print(fwd)\n",
    "\n",
    "fwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n",
    "                                         use_cps=True)\n",
    "\n",
    "# leadfield matrix\n",
    "lf = load.fiff.forward_operator(fwd_fixed, src='ico-4', subjects_dir=subjects_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NCRF estimation\n",
    "\n",
    "Now that we have all the required NDvars, we can learn the neuro-current response functions [2]_ from the data. \n",
    "For this example we use a fixed mu, but for publication purposes you need to choose mu by cross-validation,\n",
    "`mu='auto'` takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RUN fit_ncrf algorithm\n",
    "print('Running source TRF estimation')\n",
    "args=(meg, [stim1, stim2], lf, noise_cov, 0, 0.8)\n",
    "\n",
    "mu = 0.0002\n",
    "kwargs = {'normalize': 'l1', 'in_place': False, 'mu': mu,\n",
    "          'verbose': True, 'n_iter': 5, 'n_iterc': 10, 'n_iterf': 100}\n",
    "\n",
    "model = fit_ncrf(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned kernel/filter can be accessed as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.h\n",
    "print(h) # two sets of NCRFs corresponding to two different stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[0].x.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualization can be done eelbrain plot functions. We only plot `h[0]` here, please try `h[1]` when you are playing with the code.\n",
    "\n",
    "### pro tip:\n",
    "since the estimates are sparse over cortical mantle, you can smooth the ncrfs over sources to make the visualization easy (see third line of the following code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is for plotting in jupyter notebook, \n",
    "# for ipython we can use interactive time-liked plots (commented lines)\n",
    "h0 = h[0].smooth('source', 0.01, 'gaussian')\n",
    "p = plot.Butterfly(h0)\n",
    "# eelbrain.configure(frame=False)\n",
    "h_binned = h0.bin(0.1, 0.1, 0.4, 'extrema')\n",
    "sp = plot.brain.SequencePlotter()\n",
    "sp.set_brain_args(surf='inflated')\n",
    "sp.add_ndvar(h_binned)\n",
    "p = sp.plot_table(view='lateral')\n",
    "\n",
    "## for interactive session\n",
    "# bp = plot.brain.brain(h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    ".. [1] Tadel F, Baillet S, Mosher JC, Pantazis D, Leahy RM. Brainstorm: A User-Friendly Application for MEG/EEG Analysis. Computational Intelligence and Neuroscience, vol. 2011, Article ID 879716, 13 pages, 2011. doi:10.1155/2011/879716\n",
    "\n",
    ".. [2] Das P, Brodbeck C, Simon JZ, Babadi B. Neuro-Current Response Functions: A Unified Approach to MEG Source Analysis under the Continuous Stimuli Paradigm. BioRxiv. 2019. doi:10.1101/761999"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
